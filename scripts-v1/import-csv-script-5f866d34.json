{
  "schemaVersion": "2.0.0",
  "id": "5f866d34-4b29-464b-9c88-dbb2125d033d",
  "title": "Import CSV Script",
  "status": "EDITABLE",
  "version": null,
  "previousVersion": null,
  "metadata": {
    "index": "0",
    "createdAt": "1970-01-01T00:00:00.000Z",
    "lastUpdatedAt": "1970-01-01T00:00:00.000Z",
    "createdByRoleId": "44444444-4444-4444-4444-444444444444",
    "lastUpdatedByRoleId": "44444444-4444-4444-4444-444444444444",
    "editors": [],
    "typeIndex": "0"
  },
  "sourceInfo": {},
  "fields": {},
  "kind": "INSTANCE",
  "type": {
    "ref": {
      "id": "6130dcfd-5fed-4a6c-90da-4ac07852f91f"
    }
  },
  "content": {
    "type": "SCRIPT_CODE",
    "value": {
      "scriptCode": "import pandas as pd\nimport numpy as np\nfrom typing import Dict, Any\nfrom datetime import datetime, timezone, timedelta\nimport re\n\n\ndef vectorized_clean_string_column(series: pd.Series) -> pd.Series:\n    \"\"\"\n    Vectorized string cleaning for an entire column.\n    \"\"\"\n    # Strip whitespace from all strings at once\n    cleaned = series.astype(str).str.strip()\n    # Replace empty strings with None\n    cleaned = cleaned.replace('', None)\n    # Convert 'nan' strings to None\n    cleaned = cleaned.replace('nan', None)\n    return cleaned\n\n\ndef vectorized_clean_datetime_column(series: pd.Series) -> pd.Series:\n    \"\"\"\n    Vectorized datetime cleaning and UTC conversion for an entire column.\n    Handles cases where pd.to_datetime fails gracefully.\n    \"\"\"\n    # Convert to string and clean up common null representations\n    str_series = series.astype(str)\n    str_series = str_series.replace(['nan', 'NaT', '', 'None'], pd.NA)\n    \n    # Create mask for non-null values\n    mask = pd.notna(str_series)\n    \n    if not mask.any():\n        # If all values are null, return a series of None\n        return pd.Series([None] * len(series), index=series.index)\n    \n    # Try to parse datetime strings\n    try:\n        # Attempt to parse all values (including nulls)\n        dt_series = pd.to_datetime(str_series, errors='coerce')\n        \n        # Check if we got a datetime series (successful parsing)\n        if pd.api.types.is_datetime64_any_dtype(dt_series):\n            # Create a mask for successfully parsed datetime values\n            valid_mask = pd.notna(dt_series)\n            \n            if valid_mask.any():\n                # Work with a copy to avoid SettingWithCopyWarning\n                result = pd.Series([None] * len(series), index=series.index, dtype=object)\n                \n                # For valid datetimes, convert to UTC and format\n                valid_dt = dt_series[valid_mask]\n                \n                # Remove timezone info if present, then localize to UTC\n                if valid_dt.dt.tz is not None:\n                    valid_dt = valid_dt.dt.tz_convert('UTC')\n                else:\n                    valid_dt = valid_dt.dt.tz_localize('UTC')\n                \n                # Format as ISO string with Z suffix using vectorized string operations\n                formatted = valid_dt.dt.strftime('%Y-%m-%dT%H:%M:%S.%f')\n                # Trim microseconds to milliseconds and add Z\n                formatted = formatted.str[:-3] + 'Z'\n                \n                # Update result for valid values\n                result[valid_mask] = formatted\n                \n                return result\n            else:\n                # No valid datetime values parsed\n                return pd.Series([None] * len(series), index=series.index)\n        else:\n            # Parsing failed completely, return None series\n            return pd.Series([None] * len(series), index=series.index)\n            \n    except Exception as e:\n        # If any error occurs, log it and return None series\n        print(f\"Warning: Datetime parsing failed: {str(e)}\")\n        return pd.Series([None] * len(series), index=series.index)\n\n\ndef vectorized_clean_numeric_column(series: pd.Series) -> pd.Series:\n    \"\"\"\n    Vectorized numeric cleaning for an entire column.\n    \"\"\"\n    # Convert to numeric, coercing errors to NaN\n    numeric_series = pd.to_numeric(series, errors='coerce')\n    \n    # Replace inf and -inf with None\n    numeric_series = numeric_series.replace([np.inf, -np.inf], None)\n    \n    # Replace NaN with None\n    numeric_series = numeric_series.where(pd.notna(numeric_series), None)\n    \n    return numeric_series\n\n\ndef vectorized_clean_dataframe(df: pd.DataFrame, field_types: Dict[str, str]) -> pd.DataFrame:\n    \"\"\"\n    Clean entire DataFrame using vectorized operations.\n    Much faster than cell-by-cell processing.\n    \"\"\"\n    cleaned_df = df.copy()\n    \n    for column in cleaned_df.columns:\n        field_type = field_types.get(column, \"STRING\")\n        \n        if field_type == \"STRING\":\n            cleaned_df[column] = vectorized_clean_string_column(cleaned_df[column])\n        \n        elif field_type == \"DATETIME\":\n            cleaned_df[column] = vectorized_clean_datetime_column(cleaned_df[column])\n        \n        elif field_type == \"NUMBER\":\n            cleaned_df[column] = vectorized_clean_numeric_column(cleaned_df[column])\n        \n        elif field_type in [\"DATE\", \"BOOLEAN\", \"SELECT\", \"USER\", \"REFERENCE\", \"SCRIPT\"]:\n            # For these types, basic string cleaning is sufficient\n            cleaned_df[column] = vectorized_clean_string_column(cleaned_df[column])\n    \n    return cleaned_df\n\n\ndef get_template_field_types(template_entity: Dict[str, Any]) -> Dict[str, str]:\n    \"\"\"\n    Extract field types from template entity.\n    Returns a mapping of field_name -> field_type\n    \"\"\"\n    field_types = {}\n    template_fields = template_entity.get(\"fields\", {})\n    \n    for field_name, field_data in template_fields.items():\n        field_type = field_data.get(\"type\", \"STRING\")  # Default to STRING\n        field_types[field_name] = field_type\n    \n    return field_types\n\n\ndef is_empty_row(row_data: Dict[str, Any]) -> bool:\n    \"\"\"\n    Check if a row is empty (all values are None/null).\n    \"\"\"\n    for value in row_data.values():\n        if value is not None:\n            return False\n    return True\n\n\ndef main():\n    print(\"Starting CSV data import...\")\n    \n    try:\n        # Get the containing entity\n        containing_entity = seal.get_containing_entity()\n        entity_fields = containing_entity.get(\"fields\", {})\n        \n        # 1. Check for 'Imported Data' instance submission field\n        imported_data_field = entity_fields.get(\"Imported Data\")\n        if not imported_data_field:\n            raise Exception(\n                \"No 'Imported Data' field found. Please create an INSTANCE_SUBMISSION field \"\n                \"named 'Imported Data' in the containing entity before importing data.\"\n            )\n        \n        if imported_data_field.get(\"type\") != \"INSTANCE_SUBMISSION\":\n            raise Exception(\n                \"The 'Imported Data' field must be an INSTANCE_SUBMISSION field type.\"\n            )\n        \n        # Check if submission table already contains instances\n        if imported_data_field.get(\"value\") is not None and len(imported_data_field.get(\"value\", [])) > 0:\n            raise Exception(\n                \"The 'Imported Data' submission field already contains instances. \"\n                \"Please clear the field before importing new data.\"\n            )\n        \n        print(\"✓ 'Imported Data' field validated\")\n        \n        # 2. Check for published template in Template field\n        template_refs = entity_fields.get(\"Template\", {}).get(\"value\", [])\n        if not template_refs:\n            raise Exception(\n                \"No template found. Please set the 'Template' field to reference \"\n                \"a published template before importing data.\"\n            )\n        \n        template_ref = template_refs[0]\n        \n        # Check if the reference is pointing to a draft (version is null)\n        if template_ref.get(\"version\") is None:\n            raise Exception(\n                \"Template reference is pointing to a draft version. \"\n                \"Please ensure the Template field references a published version of the template.\"\n            )\n        \n        template_entity = seal.get_entity(ref=template_ref)\n        \n        # Ensure template is published (double-check)\n        if template_entity[\"status\"] != \"FINISHED\":\n            raise Exception(\n                f\"Template '{template_entity.get('title', 'Unknown')}' is not published. \"\n                f\"Please publish the template before importing data. \"\n                f\"Current status: {template_entity['status']}\"\n            )\n        \n        template_id = template_entity[\"id\"]\n        template_version = template_entity.get(\"version\")\n        \n        print(f\"✓ Using published template: {template_entity.get('title', 'Unknown')} (ID: {template_id})\")\n        \n        # 3. Check current instance submission field configuration\n        current_config = imported_data_field.get(\"config\", {})\n        current_template_ref = current_config.get(\"typeOrTemplateRef\")\n        \n        new_template_ref = {\"id\": template_id, \"version\": template_version}\n        \n        # Only update if the configuration is different\n        if current_template_ref != new_template_ref:\n            print(\"Configuring 'Imported Data' field with template...\")\n            seal.set_instance_submission_field_config(\n                field_name=\"Imported Data\",\n                template_ref=new_template_ref\n            )\n            print(\"✓ 'Imported Data' field configured with template\")\n        else:\n            print(\"✓ 'Imported Data' field already configured with the correct template\")\n        \n        # 4. Get CSV files\n        csv_files_refs = entity_fields.get(\"CSV Files\", {}).get(\"value\", [])\n        if not csv_files_refs:\n            raise Exception(\n                \"No CSV files found. Please upload CSV files to the 'CSV Files' field.\"\n            )\n        \n        print(f\"Found {len(csv_files_refs)} CSV file(s)\")\n        \n        # 5. Process each CSV file and submit rows to the submission field\n        total_rows_imported = 0\n        total_rows_skipped = 0\n        \n        for i, csv_file_ref in enumerate(csv_files_refs):\n            print(f\"\\nProcessing CSV file {i + 1}/{len(csv_files_refs)}...\")\n            \n            csv_entity = seal.get_entity(ref=csv_file_ref)\n            csv_filename = csv_entity.get(\"title\", f\"file_{i + 1}\")\n            \n            print(f\"File: {csv_filename}\")\n            \n            # Download the CSV file\n            csv_file_path = seal.download_file(ref=csv_file_ref)\n            \n            # Read CSV with pandas\n            try:\n                df = pd.read_csv(csv_file_path)\n            except Exception as e:\n                print(f\"Failed to read CSV file '{csv_filename}': {str(e)}\")\n                continue\n            \n            if df.empty:\n                print(f\"CSV file '{csv_filename}' is empty, skipping...\")\n                continue\n            \n            if len(df.columns) == 0:\n                print(f\"CSV file '{csv_filename}' has no columns, skipping...\")\n                continue\n            \n            print(f\"Loaded CSV: {len(df)} rows, {len(df.columns)} columns\")\n            \n            # Get template fields to validate against\n            template_fields = template_entity.get(\"fields\", {})\n            template_field_names = set(template_fields.keys())\n            csv_columns = set(df.columns)\n            \n            # Get field types from template for proper cleaning\n            template_field_types = get_template_field_types(template_entity)\n            \n            # Check for column mismatches\n            missing_template_fields = template_field_names - csv_columns\n            extra_csv_columns = csv_columns - template_field_names\n            \n            if missing_template_fields:\n                print(f\"Warning: Template has fields not in CSV: {missing_template_fields}\")\n            \n            if extra_csv_columns:\n                print(f\"Warning: CSV has columns not in template: {extra_csv_columns}\")\n                print(\"These columns will be ignored during import.\")\n            \n            # Filter CSV to only include columns that exist in template\n            valid_columns = [col for col in df.columns if col in template_field_names]\n            filtered_df = df[valid_columns].copy()\n            \n            print(f\"Processing {len(filtered_df)} rows with {len(valid_columns)} valid columns...\")\n            \n            # VECTORIZED CLEANING - Much faster than cell-by-cell\n            cleaned_df = vectorized_clean_dataframe(filtered_df, template_field_types)\n            \n            # Remove completely empty rows\n            row_has_data = cleaned_df.notna().any(axis=1)\n            submission_df = cleaned_df[row_has_data].copy()\n            rows_skipped = len(cleaned_df) - len(submission_df)\n            \n            print(f\"Prepared {len(submission_df)} rows for submission ({rows_skipped} empty rows skipped)\")\n            \n            # Progress indicator for large files\n            if len(submission_df) > 100:\n                print(f\"Submitting {len(submission_df)} rows in bulk...\")\n            \n            # Submit data to the instance submission field\n            if len(submission_df) > 0:\n                try:\n                    # Final cleanup to ensure JSON compatibility\n                    # Replace any remaining NaN values with None\n                    submission_df = submission_df.replace({np.nan: None})\n                    \n                    # Submit to the 'Imported Data' field\n                    print(f\"Submitting {len(submission_df)} rows to 'Imported Data' field...\")\n                    seal.submit_to_instance_submission_field(\n                        field_name=\"Imported Data\",\n                        field_values_df=submission_df,\n                        template_ref={\"id\": template_id, \"version\": template_version}\n                    )\n                    \n                    print(f\"✓ Successfully submitted {len(submission_df)} rows from '{csv_filename}'\")\n                    total_rows_imported += len(submission_df)\n                    total_rows_skipped += rows_skipped\n                    \n                except Exception as e:\n                    print(f\"Failed to submit data from '{csv_filename}': {e}\")\n                    continue\n            else:\n                print(f\"No data to import from '{csv_filename}' (all rows were empty)\")\n                total_rows_skipped += len(df)\n        \n        # Summary\n        print(f\"\\n=== Import Summary ===\")\n        print(f\"✓ Total rows imported: {total_rows_imported}\")\n        print(f\"Total rows skipped: {total_rows_skipped}\")\n        print(f\"Files processed: {len(csv_files_refs)}\")\n        \n        if total_rows_imported > 0:\n            print(f\"✓ Data successfully imported to 'Imported Data' field using template '{template_entity.get('title', 'Unknown')}'\")\n        else:\n            print(\"⚠ No data was imported. Please check your CSV files and template configuration.\")\n        \n    except Exception as e:\n        print(f\"❌ Error: {str(e)}\")\n        raise\n\nmain()"
    }
  }
}
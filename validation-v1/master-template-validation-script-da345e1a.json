{
  "schemaVersion": "2.0.0",
  "id": "da345e1a-71fd-425f-8bcd-0f6982921789",
  "title": "Master Template Validation Script",
  "status": "EDITABLE",
  "version": null,
  "previousVersion": null,
  "metadata": {
    "index": "0",
    "createdAt": "1970-01-01T00:00:00.000Z",
    "lastUpdatedAt": "1970-01-01T00:00:00.000Z",
    "createdByRoleId": "44444444-4444-4444-4444-444444444444",
    "lastUpdatedByRoleId": "44444444-4444-4444-4444-444444444444",
    "editors": [],
    "typeIndex": "0"
  },
  "sourceInfo": {},
  "fields": {},
  "kind": "INSTANCE",
  "type": {
    "ref": {
      "id": "0b207d85-cb9e-4a4f-9d78-7c73834dae31"
    }
  },
  "content": {
    "type": "SCRIPT_CODE",
    "value": {
      "scriptCode": "import asyncio\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Create a thread pool executor for running blocking I/O operations in a non-blocking way\nexecutor = ThreadPoolExecutor()\n\n\n# --- Async I/O wrapper for blocking functions ---\nasync def run_blocking(fn, *args, **kwargs):\n    \"\"\"\n    Run a blocking function asynchronously using a thread executor.\n    Useful for APIs that do not support asyncio.\n    \"\"\"\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(executor, lambda: fn(*args, **kwargs))\n\n\n# --- Core validation workflow ---\nasync def validate_change_set_async():\n    \"\"\"\n    Orchestrates the validation of all relevant templates in the current change set.\n    Collects templates directly or indirectly (via backlink references) requiring validation,\n    then runs each through the validation process.\n    \"\"\"\n    used_input_ids = []\n    templates_to_validate = []\n    template_source_map = {}  # Maps template ID to a list of referring script/template titles\n\n    # Retrieve all entities affected by the current change set\n    change_set_info = await run_blocking(seal.get_change_set_for_containing_entity)\n    change_set_entity_refs = change_set_info.get('entityRefs', [])\n\n    for entity_ref in change_set_entity_refs:\n        entity = await run_blocking(seal.get_entity, ref=entity_ref)\n        if not entity:\n            continue\n\n        kind = entity.get('kind', \"\")\n        fields = entity.get('fields', {})\n        entity_content_type = entity.get('content', {}).get('type', \"\")\n        \n        if kind == 'TEMPLATE' and 'Uses that require validation' in fields:\n            \n            # Find entities that reference this template and might need validation\n            print(fields['Uses that require validation'])\n            for reference in fields['Uses that require validation']['value']:\n                referenced_entity = await run_blocking(seal.get_entity, reference['id'])\n                fields = referenced_entity.get('fields', {})\n\n                for field_def in fields.values():\n                \n                    if field_def.get('type') != 'INSTANCE_SUBMISSION':\n                        continue\n\n                    config = field_def.get('config', {})\n                    ref_config = config.get('typeOrTemplateRef', {})\n                    if ref_config.get('id') != entity['id']:\n                        continue\n\n                    # Only validate entities using the 'activeVersion' placeholder\n                    if ref_config.get('placeholderValue') == 'activeVersion':\n                        if is_template_validation_required(referenced_entity):\n                            templates_to_validate.append(referenced_entity)\n                            template_source_map.setdefault(referenced_entity['id'], []).append(entity.get('title'))\n                        break\n\n\n        elif is_template_validation_required(entity):\n            templates_to_validate.append(entity)\n            continue\n\n        elif kind == 'INSTANCE' and entity_content_type == 'SCRIPT_CODE':\n            # For script entities, check what templates they affect\n            script_title = entity.get('title', 'Unnamed Script')\n            script_referenced_by = await run_blocking(seal.get_live_backlinks, entity['id'])\n\n            for reference_id in script_referenced_by:\n                referenced_entity = await run_blocking(seal.get_entity, reference_id)\n                if is_template_validation_required(referenced_entity):\n                    templates_to_validate.append(referenced_entity)\n                    template_source_map.setdefault(reference_id, []).append(script_title)\n\n\n    templates_to_validate = list({t['id']: t for t in templates_to_validate}.values())\n\n    # Validate all collected templates concurrently\n    tasks = [\n        validate_single_entity(entity, used_input_ids)\n        for entity in templates_to_validate\n    ]\n\n    results = await asyncio.gather(*tasks)\n    validation_status_list = [r for r in results if r]\n    return validation_status_list, used_input_ids, template_source_map\n\n\n# --- Template-level validation ---\nasync def validate_single_entity(entity, used_input_ids):\n    \"\"\"\n    Validate a single template by running all associated test cases.\n    Returns a structured summary including pass/fail and error details.\n    \"\"\"\n    entity_id = entity['id']\n    entity_title = entity['title']\n    test_suite = await run_blocking(get_test_suite, entity)\n\n    if not test_suite:\n        return build_skipped(entity_id, entity_title, 'Test suite not found')\n\n    tests = test_suite.get('fields', {}).get('Test List', {}).get('value', [])\n    if not tests:\n        return build_skipped(entity_id, entity_title, 'Test suite contains no test references')\n\n    created_test_instance_ids = []\n    test_results = []\n    for test_ref in tests:\n        result = await run_test_case_async(entity_id, test_ref, created_test_instance_ids, used_input_ids)\n        test_results.append(result)\n\n    tests_passed = sum(1 for r in test_results if r['result'] == 'pass')\n    tests_run = len(test_results)\n    status = 'pass' if tests_run == tests_passed else 'fail' if tests_passed == 0 else 'partial'\n\n    return {\n        'templateId': entity_id,\n        'templateTitle': entity_title,\n        'validationStatus': status,\n        'testsRun': tests_run,\n        'testsPassed': tests_passed,\n        'results': test_results,\n        'errors': []\n    }\n\n\n# --- Test case execution ---\nasync def run_test_case_async(entity_id, test_ref, created_ids, used_input_ids):\n    \"\"\"\n    Executes a single test case against a test instance derived from the template.\n    Applies inputs, runs embedded scripts, compares expected vs actual outputs.\n    \"\"\"\n    error_list = []\n    test_instance = await run_blocking(seal.create_test_instance_from_template, entity_id)\n    test_instance_id = test_instance['id']\n    created_ids.append(test_instance_id)\n\n    try:\n        test_entry = await run_blocking(seal.get_entity, ref=test_ref)\n        expected_result = test_entry.get('fields', {}).get('Test Pass', {}).get('value')\n        test_title = test_entry['title']\n        test_purpose = test_entry.get('fields', {}).get('Test Purpose', {}).get('value')\n    except Exception as e:\n        return format_result('unknown', 'None', 'None', [{'errorSummary': 'Failed to retrieve test entity', 'errorDetails': str(e)}])\n\n    try:\n        inputs_ref = test_entry.get('fields', {}).get('Test Input', {}).get('value', [{}])[0]\n        used_input_ids.append(inputs_ref['id'])\n        inputs = await run_blocking(seal.get_entity, ref=inputs_ref)\n        referenced_input_ids = await run_blocking(apply_test_inputs, test_instance_id, inputs.get('fields', {}), error_list)\n    except Exception as e:\n        error_list.append({'errorSummary': 'Failed to retrieve or apply test inputs', 'errorDetails': str(e)})\n        referenced_input_ids = set()\n\n    try:\n        await run_blocking(seal.run_embedded_scripts, test_instance_id)\n        post_instance = await run_blocking(seal.get_entity, test_instance_id)\n    except Exception as e:\n        return format_result(test_title, test_purpose, 'fail', [{'errorSummary': 'Script run failed', 'errorDetails': str(e)}])\n\n    try:\n        expected_ref = test_entry.get('fields', {}).get('Test Output', {}).get('value', [{}])[0]\n        expected_outputs = await run_blocking(seal.get_entity, ref=expected_ref)\n        output_errors = compare_outputs(expected_outputs.get('fields', {}), post_instance)\n        error_list.extend(output_errors)\n    except Exception as e:\n        error_list.append({'errorSummary': 'Failed to retrieve or compare outputs', 'errorDetails': str(e)})\n\n    # Revert inputs to clean state\n    for entity_id in referenced_input_ids:\n        try:\n            await run_blocking(seal.revert_entities, [entity_id])\n        except Exception as e:\n            if 'Entity is not in draft' not in str(e):\n                error_list.append({\n                    'errorSummary': f'Failed to revert draft for input entity {entity_id}',\n                    'errorDetails': str(e)\n                })\n\n    result = 'pass' if not error_list else 'fail'\n    expected_to_pass = expected_result is True\n    matches = (result == 'pass') if expected_to_pass else (result == 'fail')\n    final_result = 'pass' if matches else 'fail'\n\n    if not matches:\n        error_list.append({\n            'errorSummary': 'Result did not match expected outcome',\n            'errorDetails': f'Expected to {\"pass\" if expected_to_pass else \"fail\"}, got {result}'\n        })\n\n    return format_result(test_title, test_purpose, final_result, error_list, expected_to_pass)\n\n\n# --- Helpers ---\ndef build_skipped(template_id, title, reason):\n    return {\n        'templateId': template_id,\n        'templateTitle': title,\n        'validationStatus': 'skipped',\n        'testsRun': 0,\n        'testsPassed': 0,\n        'results': [],\n        'errors': [{'errorSummary': reason, 'errorDetails': ''}]\n    }\n\n\ndef is_template_validation_required(entity):\n    return (\n        entity.get('kind') == 'TEMPLATE'\n        and entity.get('fields', {}).get('Requires Validation', {}).get('value') is True\n    )\n\n\ndef get_test_suite(entity):\n    test_suite_ref = entity.get('fields', {}).get('Test Suite', {}).get('value', [{}])[0]\n    if not test_suite_ref:\n        return None\n    try:\n        return seal.get_entity(ref=test_suite_ref)\n    except Exception:\n        return None\n\n\ndef apply_test_inputs(test_instance_id, inputs_fields, error_list):\n    instance_fields = seal.get_entity(test_instance_id).get('fields', {})\n    referenced_ids = set()\n\n    for field, data in inputs_fields.items():\n        if field not in instance_fields:\n            error_list.append({'errorSummary': f'Missing field: {field}', 'errorDetails': f'{field} not found in instance'})\n            continue\n\n        try:\n            if data['type'] == 'INSTANCE_SUBMISSION':\n                for entity_ref in data['value']:\n                    if entity_ref and 'id' in entity_ref:\n                        referenced_ids.add(entity_ref['id'])\n                submission_to_submission(test_instance_id, data['value'], field)\n\n            elif data['type'] == 'REFERENCE':\n                ref = data.get('value')\n                if ref and 'id' in ref:\n                    referenced_ids.add(ref['id'])\n                seal.update_field_value_in_entity(test_instance_id, field, ref)\n\n            else:\n                seal.update_field_value_in_entity(test_instance_id, field, data.get('value'))\n\n        except Exception as e:\n            error_list.append({'errorSummary': f'Failed to update field: {field}', 'errorDetails': str(e)})\n\n    return referenced_ids\n\n        \n\ndef compare_outputs(expected, actual_entity):\n    errors = []\n    actual_output_fields = actual_entity.get('fields', {})\n    actual_id = actual_entity['id']\n\n    for field, expected_data in expected.items():\n        expected_value = expected_data.get('value')\n        actual_field_data = actual_output_fields.get(field, {})\n        actual_value = actual_field_data.get('value')\n        field_type = actual_field_data.get('type')\n\n        if field == 'Upcoming Status':\n            errors += compare_upcoming_status(actual_id, expected_value)\n            continue\n\n        if field_type == 'REFERENCE':\n            errors += compare_reference_entity(field, expected_value[0], actual_value[0])\n            continue\n\n        errors += compare_standard_field(field, expected_value, actual_value)\n\n    return errors\n\n\ndef compare_upcoming_status(actual_id, expected_status):\n    try:\n        upcoming_info = seal.get_upcoming_version_info(actual_id)\n        actual_status = upcoming_info.get('statusTag').get('text')\n\n        if actual_status != expected_status:\n            return [{\n                'errorSummary': f'Upcoming status mismatch',\n                'errorDetails': f'Expected: {expected_status}, Got: {actual_status}'\n            }]\n    except Exception as e:\n        return [{\n            'errorSummary': 'Failed to fetch upcoming version info',\n            'errorDetails': str(e)\n        }]\n    \n    return []\n\n\ndef compare_reference_entity(field, expected_ref, actual_ref):\n    errors = []\n\n    try:\n        expected_entity = seal.get_entity(ref = expected_ref) if expected_ref else {}\n        actual_entity = seal.get_entity(ref = actual_ref) if actual_ref else {}\n        expected_fields = expected_entity.get('fields', {})\n        actual_fields = actual_entity.get('fields', {})\n        actual_entity_id = actual_entity['id']\n\n        for subfield, expected_subdata in expected_fields.items():\n            expected_subvalue = expected_subdata.get('value')\n            actual_subvalue = actual_fields.get(subfield, {}).get('value')\n\n            if subfield == 'Upcoming Status':\n                errors += compare_upcoming_status(actual_entity_id, expected_subvalue)\n                continue\n\n            if actual_subvalue != expected_subvalue:\n                errors.append({\n                    'errorSummary': f'Mismatch in referenced field: {field}.{subfield}',\n                    'errorDetails': f'Expected: {expected_subvalue}, Got: {actual_subvalue}'\n                })\n\n    except Exception as e:\n        errors.append({\n            'errorSummary': f'Failed to fetch or compare referenced entities for field: {field}',\n            'errorDetails': str(e)\n        })\n\n    return errors\n\n\ndef compare_standard_field(field, expected_value, actual_value):\n    if expected_value != actual_value:\n        return [{\n            'errorSummary': f'Mismatch in field: {field}',\n            'errorDetails': f'Expected: {expected_value}, Got: {actual_value}'\n        }]\n    return []\n\n\n\n\ndef format_result(test_title, test_purpose, result, error_list, expected_to_pass=True):\n    return {\n        'testTitle': test_title,\n        'testPurpose': test_purpose,\n        'result': result,\n        'expectedToPass': expected_to_pass,\n        'errorList': error_list\n    }\n\n\ndef submission_to_submission(test_entity_id, submission_field_value_from, submission_field_to):\n    values = []\n    \n    for entity_ref in submission_field_value_from:\n        entity = seal.get_entity(ref=entity_ref)\n        fields = entity['fields']\n\n        # Collect a row dict with field names/values\n        row = {field_name: field_data.get(\"value\") for field_name, field_data in fields.items()}\n\n        values.append(row)\n\n    df = pd.DataFrame(values)\n    \n    seal.submit_to_instance_submission_field_in_entity(\n        test_entity_id,\n        submission_field_to,\n        field_values_df=df\n    )\n\n\ndef format_validation_summary(status_list, source_map=None):\n    lines = []\n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    lines.append(f\"### âœ… Validation Summary\\n*Last run: `{timestamp}`*\\n\")\n\n    if not status_list:\n        lines.append(\"â• No templates found in the change set.\")\n        return \"\\n\".join(lines)\n\n    all_skipped = all(t['validationStatus'] == 'skipped' for t in status_list)\n    if all_skipped:\n        lines.append(\"â• No templates required validation in this change set.\")\n        return \"\\n\".join(lines)\n\n    for template in status_list:\n        template_id = template['templateId']\n        source_scripts = source_map.get(template_id, []) if source_map else []\n        source_note = f\" _(Referenced by: {', '.join(source_scripts)})_\" if source_scripts else \"\"\n\n        header = f\"#### ðŸ” {template['templateTitle']} - `{template['validationStatus'].upper()}`{source_note}\"\n        lines.append(header)\n\n        if template['validationStatus'] == 'skipped':\n            reason = template['errors'][0]['errorSummary'] if template['errors'] else \"No validation performed\"\n            lines.append(f\"â†ªï¸ *Skipped:* {reason}\")\n            continue\n\n        lines.append(f\"*Tests passed:* {template['testsPassed']} / {template['testsRun']}\")\n\n        for test in template['results']:\n            status_emoji = \"âœ”ï¸\" if test['result'] == 'pass' else \"âŒ\"\n            line = f\"- {status_emoji} **{test['testTitle']} - {test['testPurpose']}**\"\n\n            if not test.get('expectedToPass', True):\n                line += \" _(Expecting 1 failure for successful test)_\"\n\n            lines.append(line)\n\n            for error in test['errorList']:\n                lines.append(f\"  - _{error['errorSummary']}_: {error['errorDetails']}\")\n\n        lines.append(\"\")  # Add spacing between templates\n\n    return \"\\n\".join(lines)\n\n\n\n\n# --- Run main async validation and post-processing ---\nasync def main():\n    status_summary, used_input_ids, template_source_map = await validate_change_set_async()\n    summary = format_validation_summary(status_summary, template_source_map)\n    print(summary)\n\n    all_passed = (\n        len(status_summary) > 0 and\n        all(t['validationStatus'] == 'pass' for t in status_summary if t['validationStatus'] != 'skipped')\n    )\n\n    try:\n        await run_blocking(seal.update_field_value, 'Validation Passed', all_passed)\n    except Exception:\n        pass\n\n    entity = await run_blocking(seal.get_containing_entity)\n    existing_log = entity.get('fields', {}).get('Validation Log', {}).get('value', '')\n    new_log = summary + \"\\n\\n\" + (existing_log or \"\")\n    await run_blocking(seal.update_field_value, 'Validation Log', new_log)\n\n    # Revert any input drafts if required\n    for entity_id in set(used_input_ids):  # Use set to avoid duplicates\n        try:\n            await run_blocking(seal.revert_entities, [entity_id])\n        except Exception as e:\n            error_msg = str(e)\n            if 'Entity is not in draft' not in error_msg:\n                error_list.append({\n                    'errorSummary': f'Failed to revert draft for input entity {entity_id}',\n                    'errorDetails': error_msg\n                })\n\n# Trigger the async entrypoint\nasyncio.run(main())\n\n\n\n"
    }
  }
}